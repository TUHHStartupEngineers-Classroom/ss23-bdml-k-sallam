{"title":"02 Supervised ML - Regression","markdown":{"yaml":{"title":"02 Supervised ML - Regression","date":"2023-06-08","output":{"html_document":{"toc":"yes","theme":"flatly","highlight":"tango","code_folding":"hide","df_print":"paged"},"pdf_document":{"toc":"yes"}}},"headingText":"Challenge Summary","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n    eval = FALSE,\n    message = FALSE,\n    warning = FALSE\n    )\n```\n\n\n\n\\#Which Bike Categories are in high demand?\n\\#Which Bike Categories are under represented?\n  \n__Goal__\n\n\\#Use a pricing algorithm to determine a new product price in a category gap\n\n\n# Procedure\n\n  - 1.Get the ingredients __(recipe())__: specify the response variable and predictor variables\n  - 2.Write the recipe __(step_xxx())__: define the pre-processing steps, such as imputation\n  - 3.creating __dummy variables__, scaling, and more\n  - 4.Prepare the recipe __(prep())__: provide a dataset to base each step on\n  - 5.Bake the recipe __(bake())__: apply the pre-processing steps to your datasets\n  - 6.Create a workflow __(workflow())__: Add models add_model() and the recipe add_recipe()\n  - 7.Predict the price of a new model __fit()__ and __predict()__\n\n\n# Libraries\n\nLoad the following libraries. \n\n\n```{r}\nlibrary(tidyverse)\nlibrary(parsnip)\nlibrary(recipes)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(rpart.plot)\nlibrary(broom.mixed)\nlibrary(rstanarm)\nlibrary(dials)\nlibrary(workflows)\nlibrary(vip)\nlibrary(janitor)\n```\n\n\n# Data\n\nWe will be using bike features data table\n\n- `price`: The target element, we want to predict bike price correctly\n- `category`: Bikes family which will be studied\n- `bike components manufacturere`: Components which will help predicting the price depending on the manufacturer\n- `Weight`: Component which is also involved during price prediction.\n- `Model`: Bike model, will be considered as an ID for a certain bike.\n- `Frame material`: will be used as Carbon by default\n\n```{r}\n# Bike features\nbike_features_tbl <- readRDS(\"Data/Data/bike_features_tbl.rds\")\nglimpse(bike_features_tbl)\n```\n\n## Step 1 - re-organize the data set.\n\n```{r}\n#Define the category which shall be analyzed\ncategory <- \"category_2\"\npredictors_filter = 7\n```\n\n```{r}\n# Apply your data transformation skills!\nbike_features_tbl_r <- bike_features_tbl %>% \n                       select(price, model:weight , category,`Rear Derailleur`, `Saddle`, `Shift Lever`) %>%\n                       mutate_all(funs(replace(., .==\"\", \"N/A\"))) %>%\n                       mutate(id = row_number())%>%\n                       mutate(id_1 = row_number())%>%\n                       mutate(id_2 = row_number())%>%\n                       mutate(id_3 = row_number())%>%\n                       pivot_wider(names_from = `Saddle`, values_from =id_3, names_repair = \"unique\") %>%\n                       pivot_wider(names_from = `Shift Lever`, values_from =id_2, names_repair = \"unique\") %>%\n                       pivot_wider(names_from = `Rear Derailleur`, values_from =id_1, names_repair = \"unique\") %>%\n                       select(-`N/A...59`, -`N/A...60`, -`N/A...133` ) %>%\n                       mutate_all(funs(replace_na(.,0)))\n\nbike_features_tbl_r$price <- as.integer(bike_features_tbl_r$price)\nbike_features_tbl_r$weight <- as.double(bike_features_tbl_r$weight)\n\nbike_features_tbl_r_t <-bike_features_tbl_r %>%\n                       select(-(price:id)) %>% \n                       mutate_all(funs(replace(.,.>1,1))) %>%\n                       mutate_if(is.character,as.numeric) %>%\n                       add_column(test_col = \"Don't care\", .before = TRUE) %>%\n                       adorn_totals(where =\"row\",name = \"Total\") %>%\n                       select(-test_col)  \n\n\nff = data.frame(dummy =1:231)\nfor(i in 1:ncol(bike_features_tbl_r_t)) {       # for-loop over columns\n\n  if (bike_features_tbl_r_t[232,i] >= predictors_filter)\n  {\n    ff <- ff%>% add_column(bike_features_tbl_r_t[1:231,i],.after = TRUE)  \n  }\n}\n\nbike_features_tbl_r_t <- ff %>% select(-dummy)\n\nbike_features_tbl_r_tt <- bike_features_tbl_r_t  %>% add_column(.data = bike_features_tbl_r %>% \n                                                                  select(price:id))\nbike_features_tbl_r_tt\n# Output: bike_features_tbl_r_tt\n```\n\n\n## Step 2 - TRAINING & TEST SETS\n  - `prop`: split the data according to a given percentage to train-test sets\n  - `strata`: consider different categories when dividing the data set\n  \n\n```{r}\n# Fix the random numbers by setting the seed \n# This enables the analysis to be reproducible when random numbers are used \nset.seed(seed = 1113)\n# Put 3/4 of the data into the training set \nsplit_obj <- rsample::initial_split(bike_features_tbl_r_tt, prop   = 0.75, \n                                    strata = \"category_2\")\n# Create data frames for the two sets:\ntrain_data <- training(split_obj) \ntest_data  <- testing(split_obj)\n```\n\n## Step 3 - Prepare model recipe \n\n```{r}\n\nbikes_data_set <- \n                  recipe(price ~ ., data = train_data) %>% \n                  step_rm(model_year, weight) %>%\n                  update_role(model, category_2, id, new_role = \"ID\")%>%\n                  step_dummy(all_nominal(), -all_outcomes()) %>%\n                  prep()\n\nbikes_data_set\n# Output: bikes_data_set\n```\n\n## Step 4 - Bake data sets according to the recipe \n\n\n```{r}\ntrain_transformed_tbl <- bake(bikes_data_set, train_data)\ntest_transformed_tbl  <- bake(bikes_data_set, test_data)\n\ntrain_transformed_tbl\n```\n\n\n\n```{r}\ntest_transformed_tbl\n```\n\n## Step 5 - Select model and engine to analyze the data\n\n```{r}\nlr_mod_b <- linear_reg(mode = \"regression\") %>%  set_engine(\"lm\") \n\nlr_mod_b\n```\n\n## Step 6 - Select work flow\n\n\n```{r}\nbikes_workflow <- \n  workflow() %>% \n  add_model(lr_mod_b) %>% \n  add_recipe(bikes_data_set)\n\nbikes_workflow\n```\n\n\n## Step 7 - Fit the model, to determine model's governing equation\n\n```{r}\n# Use purrr to map\nbikes_fit <- \n  bikes_workflow %>% \n  fit(data = train_data)\n\nbikes_fit\n# Output: bikes_fit \n```\n\n\n```{r}\n# Plot the given model\n\n  bikes_fit %>% pull_workflow_fit() %>% \n  tidy() %>%\n  arrange(p.value) %>% \n\n mutate(term = as_factor(term) %>% fct_rev()) %>%\n  \n  ggplot(aes(x = estimate, y = term)) +\n  geom_point(color = \"#2dc6d6\", size = 3) +\n  ggrepel::geom_label_repel(aes(label = scales::dollar(estimate, accuracy = 1, suffix = \" €\", prefix = \"\")),\n                            size = 4, fill = \"#272A36\", color = \"white\") +\n  scale_x_continuous(labels = scales::dollar_format(suffix = \" €\", prefix = \"\")) +\n  labs(title = \"Linear Regression: Feature Importance\",\n       subtitle = \"Model 01: Simple lm Model\") \n  \n```\n\n\n## Step 8 - Prepare for model prediction\n\n```{r}\n# Generalized into a function\ncalc_metrics <- function(model, new_data = test_tbl) {\n  \n  model %>%\n    predict(new_data = new_data) %>%\n    \n    bind_cols(new_data %>% select(price)) %>%\n    yardstick::metrics(truth = price, estimate = .pred)\n  \n}\n```\n\n## Step 9 - Evaluate the predicted resutls\n\n```{r}\nbikes_fit %>% calc_metrics(train_data)\n```\n\n\n## Test with model: __glmnet__ , with the same steps!\n\n```{r}\n#init model:\nset.seed(1234)\n\nlm_model_glment <-linear_reg(mode    = \"regression\", \n                                     penalty = 10, \n                                     mixture = 0.1) %>%\n    set_engine(\"glmnet\")\nlm_model_glment\n```\n```{r}\nbikes_workflow_glment <- \n  workflow() %>% \n  add_model(lm_model_glment) %>% \n  add_recipe(bikes_data_set)\n\nbikes_workflow_glment\n```\n\n```{r}\nbikes_fit_glment <- \n  bikes_workflow_glment %>% \n  fit(data = train_data)\n\nbikes_fit_glment\n```\n\n\n```{r}\nbikes_fit_glment %>% calc_metrics(train_data)\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"paged","error":false,"eval":true,"cache":true,"freeze":"auto","echo":true,"output":{"html_document":{"toc":"yes","theme":"flatly","highlight":"tango","code_folding":"hide","df_print":"paged"},"pdf_document":{"toc":"yes"}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["lightbox"],"highlight-style":"a11y-dark","toc":true,"toc-depth":2,"number-sections":true,"output-file":"02_ml_sup.html"},"language":{"code-summary":"Show the code"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.475","lightbox":"auto","theme":{"dark":"darkly","light":"flatly"},"author":"Joschka Schwarz","knitr":{"opts_chunk":{"comment":"#>","R.options":"dplyr.summarise.inform = FALSE"}},"toc-title":"Contents","number-depth":2,"title":"02 Supervised ML - Regression","date":"2023-06-08"},"extensions":{"book":{"multiFile":true}}}}}