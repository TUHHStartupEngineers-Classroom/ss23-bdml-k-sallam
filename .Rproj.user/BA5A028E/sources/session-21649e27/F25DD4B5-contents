---
title: "MyLabJournal"
output: html_document
---

<div style="padding-top: 30px;"></div>

![](images/logo.png)

# **My Lab Journal**

This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish.

## How to use

1. Accept the assignment and get your own github repo.

2. Blog/journal what you are doing in R, by editing the `.Rmd` files. 

3. See the links page for lots of helpful links on learning R.

4. Change everything to make it your own.

<!--chapter:end:index.Rmd-->

---
title: "01 Machine Learning Fundamentals"
author: "Khaled Sallam"
date: '2023-06-08'
output:
  html_document:
    toc: yes
    toc_float: yes
    df_print: paged
    collapsed: no
    number_sections: yes
    toc_depth: 3
  pdf_document:
    toc: yes
    code_folding: hide
    df_print: paged
---
---

# Libraries

Load the following libraries. 


```{r}
library(tidyverse)
library(tidyquant)
library(broom)
library(umap)
library(magrittr) 
library(dplyr)    
library(ggplot2)
```


# Data

We will be using stock prices in this analysis. Although some of you know already how to use an API to retrieve stock prices I obtained the stock prices for every stock in the S&P 500 index for you already. The files are saved in the `session_6_data` directory. 

We can read in the stock prices. The data is 1.2M observations. The most important columns for our analysis are:

- `symbol`: The stock ticker symbol that corresponds to a company's stock price
- `date`: The timestamp relating the symbol to the share price at that point in time
- `adjusted`: The stock price, adjusted for any splits and dividends (we use this when analyzing stock data over long periods of time) 


```{r}
# STOCK PRICES
sp_500_prices_tbl <- read_rds("Data/Data/sp_500_prices_tbl.rds")
sp_500_prices_tbl
```

The second data frame contains information about the stocks the most important of which are:

- `company`: The company name
- `sector`: The sector that the company belongs to

```{r}
# SECTOR INFORMATION
sp_500_index_tbl <- read_rds("raw_data/sp_500_index_tbl.rds")
sp_500_index_tbl
```


# Question

<mark>Which stock prices behave similarly?</mark>

Answering this question helps us __understand which companies are related__, and we can use clustering to help us answer it!

Even if you're not interested in finance, this is still a great analysis because it will tell you which companies are competitors and which are likely in the same space (often called sectors) and can be categorized together. Bottom line - This analysis can help you better understand the dynamics of the market and competition, which is useful for all types of analyses from finance to sales to marketing.  

Let's get started. 

## Step 1 - Convert stock prices to a standardized format (daily returns)

What you first need to do is get the data in a format that can be converted to a "user-item" style matrix. The challenge here is to connect the dots between what we have and what we need to do to format it properly.

We know that in order to compare the data, it needs to be standardized or normalized. Why? Because we cannot compare values (stock prices) that are of completely different magnitudes. In order to standardize, we will convert from adjusted stock price (dollar value) to daily returns (percent change from previous day). Here is the formula. 

$$ 
return_{daily} = \frac{price_{i}-price_{i-1}}{price_{i-1}}
$$

First, what do we have? We have stock prices for every stock in the [SP 500 Index](https://finance.yahoo.com/quote/%5EGSPC?p=%5EGSPC), which is the daily stock prices for over 500 stocks. The data set is over 1.2M observations. 

```{r}
sp_500_prices_tbl %>% glimpse()
```

Your first task is to convert to a tibble named `sp_500_daily_returns_tbl` by performing the following operations:

- Select the `symbol`, `date` and `adjusted` columns
- Filter to dates beginning in the year 2018 and beyond. 
- Compute a Lag of 1 day on the adjusted stock price. Be sure to group by symbol first, otherwise we will have lags computed using values from the previous stock in the data frame. 
- Remove a `NA` values from the lagging operation
- Compute the difference between adjusted and the lag
- Compute the percentage difference by dividing the difference by that lag. Name this column `pct_return`.
- Return only the `symbol`, `date`, and `pct_return` columns
- Save as a variable named `sp_500_daily_returns_tbl`

```{r}
sp_500_daily_returns_tbl <- select(sp_500_prices_tbl, symbol, date, adjusted)%>%
  filter(date >= as.Date("2018-01-01"))%>%
  group_by(symbol)%>%
  mutate(lag = lag(adjusted))%>%
  na.exclude()%>%
  mutate(diff = adjusted - lag)%>%
  mutate(pct_return = diff / lag)%>%
  select(symbol, date, pct_return)%>%
  ungroup()

sp_500_daily_returns_tbl

```


## Step 2 - Convert to User-Item Format

The next step is to convert to a user-item format with the `symbol` in the first column and every other column the value of the _daily returns_ (`pct_return`) for every stock at each `date`.

We're going to import the correct results first (just in case you were not able to complete the last step).



Now that we have the daily returns (percentage change from one day to the next), we can convert to a user-item format. The user in this case is the `symbol` (company), and the item in this case is the `pct_return` at each `date`. 

- Spread the `date` column to get the values as percentage returns. Make sure to fill an `NA` values with zeros. 
- Save the result as `stock_date_matrix_tbl`

```{r}
# Convert to User-Item Format
stock_date_matrix_tbl <- sp_500_daily_returns_tbl%>%
  spread(date, pct_return, fill = 0)%>%
  ungroup()

stock_date_matrix_tbl
# Output: stock_date_matrix_tbl
```



## Step 3 - Perform K-Means Clustering

Next, we'll perform __K-Means clustering__. 

We're going to import the correct results first (just in case you were not able to complete the last step).
```{r}
stock_date_matrix_tbl <- read_rds("raw_data/stock_date_matrix_tbl.rds")
```


```{r}
sp_500_daily_returns_tbl <- read_rds("Data/Data/sp_500_prices_tbl.rds")
sp_500_daily_returns_tbl
```

Beginning with the `stock_date_matrix_tbl`, perform the following operations:

- Drop the non-numeric column, `symbol`
- Perform `kmeans()` with `centers = 4` and `nstart = 20`
- Save the result as `kmeans_obj`

```{r}
# Create kmeans_obj for 4 centers
kmeans_obj <- stock_date_matrix_tbl%>%
  subset(select = -c(symbol))%>%
  kmeans(centers = 4, nstart = 20)

```

Use `glance()` to get the `tot.withinss`. 

```{r}
# Apply glance() to get the tot.withinss
broom::glance(kmeans_obj)

```

## Step 4 - Find the optimal value of K

Now that we are familiar with the process for calculating `kmeans()`, let's use `purrr` to iterate over many values of "k" using the `centers` argument. 

We'll use this __custom function__ called `kmeans_mapper()`:

```{r}
kmeans_mapper <- function(center = 3) {
    stock_date_matrix_tbl %>%
        select(-symbol) %>%
        kmeans(centers = center, nstart = 20)
}
```

Apply the `kmeans_mapper()` and `glance()` functions iteratively using `purrr`.

- Create a tibble containing column called `centers` that go from 1 to 30
- Add a column named `k_means` with the `kmeans_mapper()` output. Use `mutate()` to add the column and `map()` to map centers to the `kmeans_mapper()` function.
- Add a column named `glance` with the `glance()` output. Use `mutate()` and `map()` again to iterate over the column of `k_means`.
- Save the output as `k_means_mapped_tbl` 


```{r}
# Use purrr to map
kmeans_mapped_tbl <- tibble(centers = 1:30)%>%
  mutate(k_means = centers %>% map(kmeans_mapper))%>%
  mutate(glance  = k_means %>% map(glance))

# Output: k_means_mapped_tbl 
```

Next, let's visualize the "tot.withinss" from the glance output as a ___Scree Plot___. 

- Begin with the `k_means_mapped_tbl`
- Unnest the `glance` column
- Plot the `centers` column (x-axis) versus the `tot.withinss` column (y-axis) using `geom_point()` and `geom_line()`
- Add a title "Scree Plot" and feel free to style it with your favorite theme

```{r}
# Visualize Scree Plot
kmeans_mapped_tbl %>%
  unnest(glance)%>%
  select(centers, tot.withinss)%>%
  
  # visualization
  ggplot(aes(centers, tot.withinss)) +
  geom_point(color = "#2DC6D6", size = 4) +
  geom_line(color = "#2DC6D6", size = 1) +
    
  # Add labels (which are repelled a little)
  ggrepel::geom_label_repel(aes(label = centers), color = "#2DC6D6") + 
      
  # Formatting
  labs(title = "Scree Plot",
  subtitle = "",
  caption = "Conclusion: Based on the Scree Plot, we select 3 clusters to segment the company base.")
```

We can see that the Scree Plot becomes linear (constant rate of change) between 5 and 10 centers for K.


## Step 5 - Apply UMAP

Next, let's plot the `UMAP` 2D visualization to help us investigate cluster assignments. 


We're going to import the correct results first (just in case you were not able to complete the last step).
```{r}
k_means_mapped_tbl <- read_rds("raw_data/k_means_mapped_tbl.rds")
```

First, let's apply the `umap()` function to the `stock_date_matrix_tbl`, which contains our user-item matrix in tibble format.

- Start with `stock_date_matrix_tbl`
- De-select the `symbol` column
- Use the `umap()` function storing the output as `umap_results`
```{r}
# Apply UMAP

umap_results <- stock_date_matrix_tbl%>%
  select(-symbol)%>%
  umap()

# Store results as: umap_results 
```

Next, we want to combine the `layout` from the `umap_results` with the `symbol` column from the `stock_date_matrix_tbl`.

- Start with `umap_results$layout`
- Convert from a `matrix` data type to a `tibble` with `as_tibble()`
- Bind the columns of the umap tibble with the `symbol` column from the `stock_date_matrix_tbl`.
- Save the results as `umap_results_tbl`.

```{r}
# Convert umap results to tibble with symbols
umap_results_tbl <- umap_results$layout%>%
  as_tibble(.name_repair = "unique")%>%
  set_names(c("x", "y"))%>%
    bind_cols(
        stock_date_matrix_tbl %>% select(symbol)
    )

# Output: umap_results_tbl
```

Finally, let's make a quick visualization of the `umap_results_tbl`.

- Pipe the `umap_results_tbl` into `ggplot()` mapping the columns to x-axis and y-axis
- Add a `geom_point()` geometry with an `alpha = 0.5`
- Apply `theme_tq()` and add a title "UMAP Projection"

```{r}
# Visualize UMAP results

umap_results_tbl %>%
    ggplot(aes(x, y)) +
    geom_point(alpha = 0.5) + 
    labs(title = "UMAP Projection") +
    theme_tq()
```

We can now see that we have some clusters. However, we still need to combine the K-Means clusters and the UMAP 2D representation. 



## Step 6 - Combine K-Means and UMAP

Next, we combine the K-Means clusters and the UMAP 2D representation

We're going to import the correct results first (just in case you were not able to complete the last step).
```{r}
k_means_mapped_tbl <- read_rds("raw_data/k_means_mapped_tbl.rds")
umap_results_tbl   <- read_rds("raw_data/umap_results_tbl.rds")
```


First, pull out the K-Means for 10 Centers. Use this since beyond this value the Scree Plot flattens. 
Have a look at the business case to recall how that works.

```{r}
# Get the k_means_obj from the 10th center

k_means_obj <- kmeans_mapped_tbl %>%
    pull(k_means) %>%
    pluck(3)
```

Next, we'll combine the clusters from the `k_means_obj` with the `umap_results_tbl`.

- Begin with the `k_means_obj`
- Augment the `k_means_obj` with the `stock_date_matrix_tbl` to get the clusters added to the end of the tibble
- Select just the `symbol` and `.cluster` columns
- Left join the result with the `umap_results_tbl` by the `symbol` column
- Left join the result with the result of `sp_500_index_tbl %>% select(symbol, company, sector)` by the `symbol` column. 
- Store the output as `umap_kmeans_results_tbl`


```{r}
  # Use your dplyr & broom skills to combine the k_means_obj with the umap_results_tbl
umap_kmeans_clusters_tbl <- k_means_obj %>% 
    augment(stock_date_matrix_tbl) %>%
    select(symbol, .cluster)
    
  umap_kmeans_results_tbl <- umap_results_tbl %>%
    left_join(umap_kmeans_clusters_tbl, by = "symbol")%>%
    left_join(sp_500_index_tbl %>% select(symbol, company, sector), by = "symbol")

 umap_kmeans_results_tbl 
```


Plot the K-Means and UMAP results.

- Begin with the `umap_kmeans_results_tbl`
- Use `ggplot()` mapping `V1`, `V2` and `color = .cluster`
- Add the `geom_point()` geometry with `alpha = 0.5`
- Apply colors as you desire (e.g. `scale_color_manual(values = palette_light() %>% rep(3))`)

```{r}
# Visualize the combined K-Means and UMAP results
umap_kmeans_results_tbl %>%
    ggplot(aes(V1, V2, color = .cluster)) +
    geom_point(alpha = 0.5)
umap_kmeans_results_tbl
```

<!--chapter:end:01_ml_fund.Rmd-->

---
title: "02 Supervised ML - Regression"
date: '2021-01-06'
output:
  html_document:
    toc: yes
    theme: flatly
    highlight: tango
    code_folding: hide
    df_print: paged
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = FALSE,
    warning = FALSE
    )
```

# Challenge Summary


\#Which Bike Categories are in high demand?
\#Which Bike Categories are under represented?
  
__Goal__

\#Use a pricing algorithm to determine a new product price in a category gap


# Procedure

  - 1.Get the ingredients __(recipe())__: specify the response variable and predictor variables
  - 2.Write the recipe __(step_xxx())__: define the pre-processing steps, such as imputation
  - 3.creating __dummy variables__, scaling, and more
  - 4.Prepare the recipe __(prep())__: provide a dataset to base each step on
  - 5.Bake the recipe __(bake())__: apply the pre-processing steps to your datasets
  - 6.Create a workflow __(workflow())__: Add models add_model() and the recipe add_recipe()
  - 7.Predict the price of a new model __fit()__ and __predict()__


# Libraries

Load the following libraries. 


```{r}
library(tidyverse)
library(parsnip)
library(recipes)
library(rsample)
library(yardstick)
library(rpart.plot)
library(broom.mixed)
library(rstanarm)
library(dials)
library(workflows)
library(vip)
library(janitor)
```


# Data

We will be using bike features data table

- `price`: The target element, we want to predict bike price correctly
- `category`: Bikes family which will be studied
- `bike components manufacturere`: Components which will help predicting the price depending on the manufacturer
- `Weight`: Component which is also involved during price prediction.
- `Model`: Bike model, will be considered as an ID for a certain bike.
- `Frame material`: will be used as Carbon by default

```{r}
# Bike features
bike_features_tbl <- readRDS("Data/Data/bike_features_tbl.rds")
glimpse(bike_features_tbl)
```

## Step 1 - re-organize the data set.

```{r}
#Define the category which shall be analyzed
category <- "category_2"
predictors_filter = 7
```

```{r}
# Apply your data transformation skills!
bike_features_tbl_r <- bike_features_tbl %>% 
                       select(price, model:weight , category,`Rear Derailleur`, `Saddle`, `Shift Lever`) %>%
                       mutate_all(funs(replace(., .=="", "N/A"))) %>%
                       mutate(id = row_number())%>%
                       mutate(id_1 = row_number())%>%
                       mutate(id_2 = row_number())%>%
                       mutate(id_3 = row_number())%>%
                       pivot_wider(names_from = `Saddle`, values_from =id_3, names_repair = "unique") %>%
                       pivot_wider(names_from = `Shift Lever`, values_from =id_2, names_repair = "unique") %>%
                       pivot_wider(names_from = `Rear Derailleur`, values_from =id_1, names_repair = "unique") %>%
                       select(-`N/A...59`, -`N/A...60`, -`N/A...133` ) %>%
                       mutate_all(funs(replace_na(.,0)))

bike_features_tbl_r$price <- as.integer(bike_features_tbl_r$price)
bike_features_tbl_r$weight <- as.double(bike_features_tbl_r$weight)

bike_features_tbl_r_t <-bike_features_tbl_r %>%
                       select(-(price:id)) %>% 
                       mutate_all(funs(replace(.,.>1,1))) %>%
                       mutate_if(is.character,as.numeric) %>%
                       add_column(test_col = "Don't care", .before = TRUE) %>%
                       adorn_totals(where ="row",name = "Total") %>%
                       select(-test_col)  


ff = data.frame(dummy =1:231)
for(i in 1:ncol(bike_features_tbl_r_t)) {       # for-loop over columns

  if (bike_features_tbl_r_t[232,i] >= predictors_filter)
  {
    ff <- ff%>% add_column(bike_features_tbl_r_t[1:231,i],.after = TRUE)  
  }
}

bike_features_tbl_r_t <- ff %>% select(-dummy)

bike_features_tbl_r_tt <- bike_features_tbl_r_t  %>% add_column(.data = bike_features_tbl_r %>% 
                                                                  select(price:id))
bike_features_tbl_r_tt
# Output: bike_features_tbl_r_tt
```


## Step 2 - TRAINING & TEST SETS
  - `prop`: split the data according to a given percentage to train-test sets
  - `strata`: consider different categories when dividing the data set
  

```{r}
# Fix the random numbers by setting the seed 
# This enables the analysis to be reproducible when random numbers are used 
set.seed(seed = 1113)
# Put 3/4 of the data into the training set 
split_obj <- rsample::initial_split(bike_features_tbl_r_tt, prop   = 0.75, 
                                    strata = "category_2")
# Create data frames for the two sets:
train_data <- training(split_obj) 
test_data  <- testing(split_obj)
```

## Step 3 - Prepare model recipe 

```{r}

bikes_data_set <- 
                  recipe(price ~ ., data = train_data) %>% 
                  step_rm(model_year, weight) %>%
                  update_role(model, category_2, id, new_role = "ID")%>%
                  step_dummy(all_nominal(), -all_outcomes()) %>%
                  prep()

bikes_data_set
# Output: bikes_data_set
```

## Step 4 - Bake data sets according to the recipe 


```{r}
train_transformed_tbl <- bake(bikes_data_set, train_data)
test_transformed_tbl  <- bake(bikes_data_set, test_data)

train_transformed_tbl
```



```{r}
test_transformed_tbl
```

## Step 5 - Select model and engine to analyze the data

```{r}
lr_mod_b <- linear_reg(mode = "regression") %>%  set_engine("lm") 

lr_mod_b
```

## Step 6 - Select work flow


```{r}
bikes_workflow <- 
  workflow() %>% 
  add_model(lr_mod_b) %>% 
  add_recipe(bikes_data_set)

bikes_workflow
```


## Step 7 - Fit the model, to determine model's governing equation

```{r}
# Use purrr to map
bikes_fit <- 
  bikes_workflow %>% 
  fit(data = train_data)

bikes_fit
# Output: bikes_fit 
```


```{r}
# Plot the given model

  bikes_fit %>% pull_workflow_fit() %>% 
  tidy() %>%
  arrange(p.value) %>% 

 mutate(term = as_factor(term) %>% fct_rev()) %>%
  
  ggplot(aes(x = estimate, y = term)) +
  geom_point(color = "#2dc6d6", size = 3) +
  ggrepel::geom_label_repel(aes(label = scales::dollar(estimate, accuracy = 1, suffix = " €", prefix = "")),
                            size = 4, fill = "#272A36", color = "white") +
  scale_x_continuous(labels = scales::dollar_format(suffix = " €", prefix = "")) +
  labs(title = "Linear Regression: Feature Importance",
       subtitle = "Model 01: Simple lm Model") 
  
```


## Step 8 - Prepare for model prediction

```{r}
# Generalized into a function
calc_metrics <- function(model, new_data = test_tbl) {
  
  model %>%
    predict(new_data = new_data) %>%
    
    bind_cols(new_data %>% select(price)) %>%
    yardstick::metrics(truth = price, estimate = .pred)
  
}
```

## Step 9 - Evaluate the predicted resutls

```{r}
bikes_fit %>% calc_metrics(train_data)
```


## Test with model: __glmnet__ , with the same steps!

```{r}
#init model:
set.seed(1234)

lm_model_glment <-linear_reg(mode    = "regression", 
                                     penalty = 10, 
                                     mixture = 0.1) %>%
    set_engine("glmnet")
lm_model_glment
```
```{r}
bikes_workflow_glment <- 
  workflow() %>% 
  add_model(lm_model_glment) %>% 
  add_recipe(bikes_data_set)

bikes_workflow_glment
```

```{r}
bikes_fit_glment <- 
  bikes_workflow_glment %>% 
  fit(data = train_data)

bikes_fit_glment
```


```{r}
bikes_fit_glment %>% calc_metrics(train_data)

<!--chapter:end:02_ml_sup.Rmd-->

---
title: "03 Automated Machine Learning with H20"
date: '2021-01-06'
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    collapsed: false
    number_sections: true
    toc_depth: 3
    code_folding: hide
---

# Automated Machine Learning with H20 (I) Challenge


## Loading libraries
```{r}
library(h2o)
library(tidyverse)
library(readxl)
library(skimr)
library(GGally)
library(ggplot2)
library(rmarkdown)
library(rsample)
library(recipes)
library(PerformanceAnalytics)
``` 


## Q1:

The answer is C
```{r}
#Q1
employee_attrition_tbl %>%
  select(Attrition, contains("income")) %>%
  plot_ggpairs(Attrition)
#The answer is C
```


## Q2:

The answer is D
```{r}
#Q2
employee_attrition_tbl %>%
  select(Attrition, contains("PercentSalaryHike")) %>%
  plot_ggpairs(Attrition)

#The answer is D
```


## Q3

The answer is B
```{r}
#Q3
employee_attrition_tbl %>%
  select(Attrition, contains("StockOptionLevel")) %>%
  plot_ggpairs(Attrition)

#The answer is B
```

## Q4

The answer is A
```{r}
#Q4

employee_attrition_tbl %>%
  select(Attrition, contains("EnvironmentSatisfaction")) %>%
  plot_ggpairs(Attrition)

#The answer is A
```

## Q5

The answer is B
```{r}
#Q5

employee_attrition_tbl %>%
  select(Attrition, contains("WorkLifeBalance")) %>%
  plot_ggpairs(Attrition)

#The answer is B
```

## Q6

The answer is A
```{r}
#Q6

employee_attrition_tbl %>%
  select(Attrition, contains("JobInvolvement")) %>%
  plot_ggpairs(Attrition)

#The answer is A
```

## Q7

The answer is B
```{r}
#Q7
employee_attrition_tbl %>%
  select(Attrition, contains("OverTime")) %>%
  plot_ggpairs(Attrition)

#The answer is B
```

## Q8

The answer is B
```{r}
#Q8
employee_attrition_tbl %>%
  select(Attrition, contains("TrainingTimesLastYear")) %>%
  plot_ggpairs(Attrition)

#The answer is B
```

## Q9
 The answer is B
```{r}
#Q9
employee_attrition_tbl %>%
  select(Attrition, contains("YearsAtCompany")) %>%
  plot_ggpairs(Attrition)
#The answer is B
```

## Q10

The answer is C
```{r}
#Q 10
employee_attrition_tbl %>%
  select(Attrition, contains("YearsSinceLastPromotion")) %>%
  plot_ggpairs(Attrition)

#The answer is C
```


# Automated Machine Learning with H20 (II) Challenge


## Loading data
```{r}
product_backorders_tbl <- read_csv("product_backorders.csv")
```



## Split into test and train
```{r}
set.seed(seed = 1113)
split_obj <- rsample::initial_split(product_backorders_tbl, prop = 0.85)
```


## Assign training and test data

```{r}
train_readable_tbl<- training(split_obj)
test_readable_tbl <- testing(split_obj)


  recipe_obj <- recipe( went_on_backorder~., data = train_readable_tbl) %>%
  step_zv(all_predictors()) %>%
  prep()

train_tbl <- bake(recipe_obj, new_data = train_readable_tbl)
test_tbl  <- bake(recipe_obj, new_data = test_readable_tbl)

 #set the predictor names
 predictors <- c("national_inv", "lead_time", "forecast_3_month", "sales_3_month")

```


## specify the response
```{r}
 response <- "went_on_backorder"
```

## Automated machine learning Model

### Initiate H2O

```{r}
h2o.init()
```


### Split data into a training and a validation data frame

```{r}
# Setting the seed is just for reproducability
 split_h2o <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.85), seed = 1234)
 train_h2o <- split_h2o[[1]]
 valid_h2o <- split_h2o[[2]]
 test_h2o  <- as.h2o(test_tbl)
```



### Set the target and predictors
```{r}
 y <- response
 x <- setdiff(names(train_h2o), y)
```

### Predicting using Leader Model

```{r}
 ?h2o.automl

 automl_models_h2o <- h2o.automl(
   x = x,
   y = y,
   training_frame    = train_h2o,
   validation_frame  = valid_h2o,
   leaderboard_frame = test_h2o,
   max_runtime_secs  = 15,
   nfolds            = 5
 )
 automl_models_h2o@leaderboard


 Model<-automl_models_h2o@leader
```


### Saving The Leader Model

```{r}
h2o.saveModel(Model,path = "ml_journal-KhaledSallam")
```

<!--chapter:end:03_ml_aut.Rmd-->

---
title: "04 Performance Measures"
date: '2021-01-06'

output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: true
    toc_depth: 3
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = FALSE,
    warning = FALSE
    )
```

```{r}
# install.packages("plotly")
library(tidyverse)
library(tidyquant)
library(broom)
library(umap)
library(readxl)
library(h2o)
library(readxl)
library(rsample)
library(recipes)
library(PerformanceAnalytics)
library(h2o)
```

```{r}

product_backorders_tbl <- read_csv("product_backorders.csv")

# Split into test and train
set.seed(seed = 1113)
split_obj <- rsample::initial_split(product_backorders_tbl, prop = 0.85)


# Assign training and test data
train_readable_tbl<- training(split_obj)
test_readable_tbl <- testing(split_obj)


  recipe_obj <- recipe( went_on_backorder~., data = train_readable_tbl) %>% 
  step_zv(all_predictors()) %>%
  prep()
  
train_tbl <- bake(recipe_obj, new_data = train_readable_tbl)
test_tbl  <- bake(recipe_obj, new_data = test_readable_tbl)

 #set the predictor names
 predictors <- c("national_inv", "lead_time", "forecast_3_month", "sales_3_month")
  
# #specify the response
 response <- "went_on_backorder"

 

 h2o.init()
 # Split data into a training and a validation data frame
 # Setting the seed is just for reproducability
 split_h2o <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.85), seed = 1234)
 train_h2o <- split_h2o[[1]]
 valid_h2o <- split_h2o[[2]]
 test_h2o  <- as.h2o(test_tbl)
 
 # Set the target and predictors
 y <- response
 x <- setdiff(names(train_h2o), y)
 
 ?h2o.automl
 
 automl_models_h2o <- h2o.automl(
   x = x,
   y = y,
   training_frame    = train_h2o,
   validation_frame  = valid_h2o,
   leaderboard_frame = test_h2o,
   max_runtime_secs  = 15,
   nfolds            = 5 
 )



leaderboard_Model <- automl_models_h2o@leaderboard

 
 Model<-automl_models_h2o@leader
 
 
   h2o.saveModel(Model,path = "ml_journal-KhaledSallam")
```

```{r}
# Leader board Visualization
automl_models_h2o@leaderboard %>% 
              as_tibble() %>% 
              select(-c(mean_per_class_error, rmse, mse))

```

```{r}
# data preparation and Visulatization

data_transformed_tbl <- automl_models_h2o@leaderboard %>%
  as_tibble() %>%
  select(-c(aucpr, mean_per_class_error, rmse, mse)) %>%
  mutate(model_type = str_extract(model_id, "[^_]+")) %>%
  slice(1:15) %>% 
  rownames_to_column(var = "rowname") %>%
  
  mutate(
  model_id   = as_factor(model_id) %>% reorder(auc),
  model_type = as.factor(model_type) 
  ) %>%
  
  pivot_longer(cols = -c(model_id, model_type, rowname),
                       names_to = "key",
                       values_to = "value",
                       names_transform = list(key = forcats::fct_inorder)
                       ) %>%
        mutate(model_id = paste0(rowname, ". ", model_id) %>% as_factor() %>% fct_rev())

# Final tibble to visualize

data_transformed_tbl
```

```{r}
## Visualization

data_transformed_tbl %>%
        ggplot(aes(value, model_id, color = model_type)) +
        geom_point(size = 3) +
        geom_label(aes(label = round(value, 2), hjust = "inward")) +
        
        # Facet to break out logloss and auc
        facet_wrap(~ key, scales = "free_x") +
        labs(title = "Leaderboard Metrics",
             subtitle = paste0("Ordered by: ", "auc"),
             y = "Model Postion, Model ID", x = "") + 
        theme(legend.position = "bottom")
```

```{r}
# Grid Search

grid_search_model <- h2o.loadModel("ml_journal-KhaledSallam/StackedEnsemble_AllModels_AutoML_20210105_234211")

grid_search_model
test_tbl

# test performance with new data output from previous test

h2o.performance(grid_search_model, newdata = as.h2o(test_tbl))

```

```{r}
# Grid Search

search_grid_01 <- h2o.grid(
  algorithm = "deeplearning",
  grid_id = "search_grid_01",
 x = x,
    y = y,
    
    # training and validation frame and crossfold validation
    training_frame   = train_h2o,
    validation_frame = valid_h2o,
    nfolds = 5,
    
    # Hyperparamters: Use deeplearning_h2o@allparameters to see all
    hyper_params = list(
        # Use some combinations (the first one was the original)
        hidden = list(c(10, 10, 10), c(50, 20, 10), c(20, 20, 20)),
        epochs = c(10, 50, 20)
    )
)

```

```{r}

search_grid_01

```

```{r}
# sort accoding to auc high to low
h2o.getGrid(grid_id = "search_grid_01", sort_by = "auc", decreasing = TRUE)

```

```{r}

search_grid_01_model_1 <- h2o.getModel("search_grid_01_model_1")
search_grid_01_model_1 %>% h2o.auc(train = T, valid = T, xval = T)

search_grid_01_model_1 %>%
    h2o.performance(newdata = as.h2o(test_tbl))

```

```{r}
# H2o Performance

#load model
stacked_ensemble_h2o <- h2o.loadModel("ml_journal-KhaledSallam/StackedEnsemble_AllModels_AutoML_20210105_234211")

performance_h2o <- h2o.performance(stacked_ensemble_h2o, newdata = as.h2o(test_tbl))

typeof(performance_h2o)
performance_h2o %>% slotNames()

performance_h2o@metrics

# Classifier Summary Metrics

h2o.auc(performance_h2o, train = T, valid = T, xval = T)
# our value is [1] 0.9037603

h2o.auc(stacked_ensemble_h2o, train = T, valid = T, xval = T)
#     train     valid      xval
# 0.9320589 0.8932458 0.8576325

h2o.giniCoef(performance_h2o)
# [1] 0.8075205

h2o.logloss(performance_h2o)
# [1] 0.2362433

# result for the training data
h2o.confusionMatrix(stacked_ensemble_h2o)
# Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.268791256959701

# result for the hold out set
h2o.confusionMatrix(performance_h2o)
# Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.25817823725732
```

```{r}

performance_tbl <- performance_h2o %>%
    h2o.metric() %>%
    as.tibble() 

performance_tbl %>% 
  glimpse()

# save our theme 
theme_new <- theme(
      legend.position  = "bottom",
      legend.key       = element_blank(),
      panel.background = element_rect(fill   = "transparent"),
      panel.border     = element_rect(color = "black", fill = NA, size = 0.5),
      panel.grid.major = element_line(color = "grey", size = 0.333)
      ) 

```

```{r}
# Visualize the trade of between the precision and the recall and the optimal threshold

performance_tbl %>%
    filter(f1 == max(f1))

performance_tbl %>%
    ggplot(aes(x = threshold)) +
    geom_line(aes(y = precision), color = "blue", size = 1) +
    geom_line(aes(y = recall), color = "red", size = 1) +
    
    # Insert line where precision and recall are harmonically optimized
    geom_vline(xintercept = h2o.find_threshold_by_max_metric(performance_h2o, "f1")) +
    labs(title = "Precision vs Recall", y = "value") +
    theme_new
```

```{r}
#ROC Plot 

path <- "/StackedEnsemble_AllModels_AutoML_20200826_112031"

load_model_performance_metrics <- function(path, test_tbl) {
    
    model_h2o <- h2o.loadModel(path)
    perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) 
    
    perf_h2o %>%
        h2o.metric() %>%
        as_tibble() %>%
        mutate(auc = h2o.auc(perf_h2o)) %>%
        select(tpr, fpr, auc)
    
}

model_metrics_tbl <- fs::dir_info(path = "ml_journal-KhaledSallam") %>%
    select(path) %>%
    mutate(metrics = map(path, load_model_performance_metrics, test_tbl)) %>%
    unnest(cols = metrics)

model_metrics_tbl %>%
    mutate(
        # Extract the model names
        path = str_split(path, pattern = "/", simplify = T)[,2] %>% as_factor(),
        auc  = auc %>% round(3) %>% as.character() %>% as_factor()
        ) %>%
    ggplot(aes(fpr, tpr, color = path, linetype = auc)) +
    geom_line(size = 1) +
    
    # just for demonstration purposes
    geom_abline(color = "red", linetype = "dotted") +
    
    theme_new +
    theme(
      legend.direction = "vertical",
      ) +
    labs(
        title = "ROC Plot",
        subtitle = "Performance of 3 Top Performing Models"
    )
```

```{r}

# Percision vs Recall plot 

# Precision vs Recall

load_model_performance_metrics <- function(path, test_tbl) {
    
    model_h2o <- h2o.loadModel(path)
    perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) 
    
    perf_h2o %>%
        h2o.metric() %>%
        as_tibble() %>%
        mutate(auc = h2o.auc(perf_h2o)) %>%
        select(tpr, fpr, auc, precision, recall)
    
}

model_metrics_tbl <- fs::dir_info(path = "ml_journal-KhaledSallam") %>%
    select(path) %>%
    mutate(metrics = map(path, load_model_performance_metrics, test_tbl)) %>%
    unnest(cols = metrics)

model_metrics_tbl %>%
    mutate(
        path = str_split(path, pattern = "/", simplify = T)[,2] %>% as_factor(),
        auc  = auc %>% round(3) %>% as.character() %>% as_factor()
    ) %>%
    ggplot(aes(recall, precision, color = path, linetype = auc)) +
    geom_line(size = 1) +
    theme_new + 
    theme(
      legend.direction = "vertical",
      ) +
    labs(
        title = "Precision vs Recall Plot",
        subtitle = "Performance of 3 Top Performing Models"
    )

```

```{r}
# getting predictions_tbl from previous session
predictions <- h2o.predict(stacked_ensemble_h2o, newdata = as.h2o(test_tbl))

typeof(predictions)
# [1] "environment"

predictions_tbl <- predictions %>% as_tibble()

# No	0.938624996	0.06137500		
# Yes	0.767573922	0.23242608		
# No	0.934670085	0.06532991		
# Yes	0.580364130	0.41963587		
# Yes	0.738244189	0.26175581		
# Yes	0.767517540	0.23248246		
# Yes	0.580365130	0.41963487		
# No	0.940360179	0.05963982		
# Yes	0.616682970	0.38331703		
# Yes	0.111431957	0.88856804	

# Gain & Lift

ranked_predictions_tbl <- predictions_tbl %>%
    bind_cols(test_tbl) %>%
    select(predict:Yes, went_on_backorder) %>%
    arrange(desc(Yes))

ranked_predictions_tbl
```

```{r}
# Gain and Lift calculations 

ranked_predictions_tbl %>%
    mutate(ntile = ntile(Yes, n = 10)) %>%
    group_by(ntile) %>%
    summarise(
        cases = n(),
        responses = sum(went_on_backorder == "Yes")
    ) %>%
    arrange(desc(ntile))

calculated_gain_lift_tbl <- ranked_predictions_tbl %>%
    mutate(ntile = ntile(Yes, n = 10)) %>%
    group_by(ntile) %>%
    summarise(
        cases = n(),
        responses = sum(went_on_backorder == "Yes")
    ) %>%
    arrange(desc(ntile)) %>%
    
    # Add group numbers (opposite of ntile)
    mutate(group = row_number()) %>%
    select(group, cases, responses) %>%
    
    # Calculations
    mutate(
        cumulative_responses = cumsum(responses),
        pct_responses        = responses / sum(responses),
        gain                 = cumsum(pct_responses),
        cumulative_pct_cases = cumsum(cases) / sum(cases),
        lift                 = gain / cumulative_pct_cases,
        gain_baseline        = cumulative_pct_cases,
        lift_baseline        = gain_baseline / cumulative_pct_cases
    )

calculated_gain_lift_tbl 
```

```{r}
# Gain Plot

gain_lift_tbl <- performance_h2o %>%
    h2o.gainsLift() %>%
    as.tibble()

## Gain Chart

gain_transformed_tbl <- gain_lift_tbl %>% 
    select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %>%
    select(-contains("lift")) %>%
    mutate(baseline = cumulative_data_fraction) %>%
    rename(gain     = cumulative_capture_rate) %>%
    # prepare the data for the plotting (for the color and group aesthetics)
    pivot_longer(cols = c(gain, baseline), values_to = "value", names_to = "key")

gain_transformed_tbl %>%
    ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +
    geom_line(size = 1.5) +
    labs(
        title = "Gain Chart",
        x = "Cumulative Data Fraction",
        y = "Gain"
    ) +
    theme_new
```

```{r}
# Lift Plot

lift_transformed_tbl <- gain_lift_tbl %>% 
    select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %>%
    select(-contains("capture")) %>%
    mutate(baseline = 1) %>%
    rename(lift = cumulative_lift) %>%
    pivot_longer(cols = c(lift, baseline), values_to = "value", names_to = "key")

lift_transformed_tbl %>%
    ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +
    geom_line(size = 1.5) +
    labs(
        title = "Lift Chart",
        x = "Cumulative Data Fraction",
        y = "Lift"
    ) +
    theme_new
```

```{r}
# Cowplot block
# Visualization

library(cowplot)
library(glue)

# set values to test the function while building it
h2o_leaderboard <- automl_models_h2o@leaderboard
newdata <- test_tbl
order_by <- "auc"
max_models <- 4
size <- 1

plot_h2o_performance <- function(h2o_leaderboard, newdata, order_by = c("auc", "logloss"),
                                 max_models = 3, size = 1.5) {
    
    # Inputs
    leaderboard_tbl <- h2o_leaderboard %>%
        as_tibble() %>%
        slice(1:max_models)
    
    newdata_tbl <- newdata %>%
        as_tibble()
    
     # Selecting the first, if nothing is provided
    order_by      <- tolower(order_by[[1]]) 
    
    # Convert string stored in a variable to column name (symbol)
    order_by_expr <- rlang::sym(order_by)

    # Turn of the progress bars ( opposite h2o.show_progress())
    h2o.no_progress()
    
    # 1. Model metrics
    
    get_model_performance_metrics <- function(model_id, test_tbl) {
        
        model_h2o <- h2o.getModel(model_id)
        perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl))
        
        perf_h2o %>%
            h2o.metric() %>%
            as.tibble() %>%
            select(threshold, tpr, fpr, precision, recall)
    }
        model_metrics_tbl <- leaderboard_tbl %>%
        mutate(metrics = map(model_id, get_model_performance_metrics, newdata_tbl)) %>%
        unnest(cols = metrics) %>%
        mutate(
          model_id = as_factor(model_id) %>% 
                      # programmatically reorder factors depending on order_by
                      fct_reorder(!! order_by_expr, 
                                  .desc = ifelse(order_by == "auc", TRUE, FALSE)),
          auc      = auc %>% 
                      round(3) %>% 
                      as.character() %>% 
                      as_factor() %>% 
                      fct_reorder(as.numeric(model_id)),
          logloss  = logloss %>% 
                      round(4) %>% 
                      as.character() %>% 
                      as_factor() %>% 
                      fct_reorder(as.numeric(model_id))
        )
    
    
    # 1A. ROC Plot
    
    p1 <- model_metrics_tbl %>%
        ggplot(aes(fpr, tpr, color = model_id, linetype = !! order_by_expr)) +
        geom_line(size = size) +
        theme_new +
        labs(title = "ROC", x = "FPR", y = "TPR") +
        theme(legend.direction = "vertical") 
        
    
    # 1B. Precision vs Recall
    
    p2 <- model_metrics_tbl %>%
        ggplot(aes(recall, precision, color = model_id, linetype = !! order_by_expr)) +
        geom_line(size = size) +
        theme_new +
        labs(title = "Precision Vs Recall", x = "Recall", y = "Precision") +
        theme(legend.position = "none") 
    
    
    # 2. Gain / Lift
    
    get_gain_lift <- function(model_id, test_tbl) {
        
        model_h2o <- h2o.getModel(model_id)
        perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) 
        
        perf_h2o %>%
            h2o.gainsLift() %>%
            as.tibble() %>%
            select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift)
        
    }
    
    gain_lift_tbl <- leaderboard_tbl %>%
        mutate(metrics = map(model_id, get_gain_lift, newdata_tbl)) %>%
        unnest(cols = metrics) %>%
        mutate(
            model_id = as_factor(model_id) %>% 
                fct_reorder(!! order_by_expr, 
                            .desc = ifelse(order_by == "auc", TRUE, FALSE)),
            auc  = auc %>% 
                round(3) %>% 
                as.character() %>% 
                as_factor() %>% 
                fct_reorder(as.numeric(model_id)),
            logloss = logloss %>% 
                round(4) %>% 
                as.character() %>% 
                as_factor() %>% 
                fct_reorder(as.numeric(model_id))
        ) %>%
        rename(
            gain = cumulative_capture_rate,
            lift = cumulative_lift
        ) 
    
    # 2A. Gain Plot
    
    p3 <- gain_lift_tbl %>%
        ggplot(aes(cumulative_data_fraction, gain, 
                          color = model_id, linetype = !! order_by_expr)) +
        geom_line(size = size,) +
        geom_segment(x = 0, y = 0, xend = 1, yend = 1, 
                     color = "red", size = size, linetype = "dotted") +
        theme_new +
        expand_limits(x = c(0, 1), y = c(0, 1)) +
        labs(title = "Gain",
             x = "Cumulative Data Fraction", y = "Gain") +
        theme(legend.position = "none")
    
    # 2B. Lift Plot
    
    p4 <- gain_lift_tbl %>%
        ggplot(aes(cumulative_data_fraction, lift, 
                          color = model_id, linetype = !! order_by_expr)) +
        geom_line(size = size) +
        geom_segment(x = 0, y = 1, xend = 1, yend = 1, 
                     color = "red", size = size, linetype = "dotted") +
        theme_new +
        expand_limits(x = c(0, 1), y = c(0, 1)) +
        labs(title = "Lift",
             x = "Cumulative Data Fraction", y = "Lift") +
        theme(legend.position = "none") 
    
    
    # Combine using cowplot
    
    # cowplot::get_legend extracts a legend from a ggplot object
    p_legend <- get_legend(p1)
    # Remove legend from p1
    p1 <- p1 + theme(legend.position = "none")
    
    # cowplot::plt_grid() combines multiple ggplots into a single cowplot object
    p <- cowplot::plot_grid(p1, p2, p3, p4, ncol = 2)
    
    # cowplot::ggdraw() sets up a drawing layer
    p_title <- ggdraw() + 
    
        # cowplot::draw_label() draws text on a ggdraw layer / ggplot object
        draw_label("H2O Model Metrics", size = 18, fontface = "bold", 
                   color = "#2C3E50")
    
    p_subtitle <- ggdraw() + 
        draw_label(glue("Ordered by {toupper(order_by)}"), size = 10,  
                   color = "#2C3E50")
    
    # Combine everything
    ret <- plot_grid(p_title, p_subtitle, p, p_legend, 
    
                     # Adjust the relative spacing, so that the legends always fits
                     ncol = 1, rel_heights = c(0.05, 0.05, 1, 0.05 * max_models))
    
    h2o.show_progress()
    
    return(ret)
    
}

automl_models_h2o@leaderboard %>%
    plot_h2o_performance(newdata = test_tbl, order_by = "logloss", 
                         size = 0.5, max_models = 4)
  

```

<!--chapter:end:04_perf_meas.Rmd-->

---
title: "05 LIME"
date: "2023-06-08"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    collapsed: false
    number_sections: true
    toc_depth: 3
    #code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```

**IMPORTANT:** You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.

This is an `.Rmd` file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a \# in front of your text, it will create a top level-header.

# My first post

Last compiled: `r Sys.Date()`

Notice that whatever you define as a top level header, automatically gets put into the table of contents bar on the left. 

## Second level header

You can add more headers by adding more hashtags. These won't be put into the table of contents

### third level header

Here's an even lower level header

# My second post (note the order)

Last compiled: `r Sys.Date()`

I'm writing this tutorial going from the top down. And, this is how it will be printed. So, notice the second post is second in the list. If you want your most recent post to be at the top, then make a new post starting at the top. If you want the oldest first, do, then keep adding to the bottom

# Adding R stuff

So far this is just a blog where you can write in plain text and serve your writing to a webpage. One of the main purposes of this lab journal is to record your progress learning R. The reason I am asking you to use this process is because you can both make a website, and a lab journal, and learn R all in R-studio. This makes everything really convenient and in the same place. 

So, let's say you are learning how to make a histogram in R. For example, maybe you want to sample 100 numbers from a normal distribution with mean = 0, and standard deviation = 1, and then you want to plot a histogram. You can do this right here by using an r code block, like this:

```{r}
samples <- rnorm(100, mean=0, sd=1)
hist(samples)
```

When you knit this R Markdown document, you will see that the histogram is printed to the page, along with the R code. This document can be set up to hide the R code in the webpage, just delete the comment (hashtag) from the cold folding option in the yaml header up top. For purposes of letting yourself see the code, and me see the code, best to keep it the way that it is. You'll learn that all of these things and more can be customized in each R code block.

<!--chapter:end:05_lime.Rmd-->

---
title: "06 Deep Learning"
date: "2023-06-08"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    collapsed: false
    number_sections: true
    toc_depth: 3
    #code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```

**IMPORTANT:** You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.

This is an `.Rmd` file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a \# in front of your text, it will create a top level-header.

# My first post

Last compiled: `r Sys.Date()`

Notice that whatever you define as a top level header, automatically gets put into the table of contents bar on the left. 

## Second level header

You can add more headers by adding more hashtags. These won't be put into the table of contents

### third level header

Here's an even lower level header

# My second post (note the order)

Last compiled: `r Sys.Date()`

I'm writing this tutorial going from the top down. And, this is how it will be printed. So, notice the second post is second in the list. If you want your most recent post to be at the top, then make a new post starting at the top. If you want the oldest first, do, then keep adding to the bottom

# Adding R stuff

So far this is just a blog where you can write in plain text and serve your writing to a webpage. One of the main purposes of this lab journal is to record your progress learning R. The reason I am asking you to use this process is because you can both make a website, and a lab journal, and learn R all in R-studio. This makes everything really convenient and in the same place. 

So, let's say you are learning how to make a histogram in R. For example, maybe you want to sample 100 numbers from a normal distribution with mean = 0, and standard deviation = 1, and then you want to plot a histogram. You can do this right here by using an r code block, like this:

```{r}
samples <- rnorm(100, mean=0, sd=1)
hist(samples)
```

When you knit this R Markdown document, you will see that the histogram is printed to the page, along with the R code. This document can be set up to hide the R code in the webpage, just delete the comment (hashtag) from the cold folding option in the yaml header up top. For purposes of letting yourself see the code, and me see the code, best to keep it the way that it is. You'll learn that all of these things and more can be customized in each R code block.

<!--chapter:end:06_dl.Rmd-->

---
title: "Class notes"
date: "2020-11-05"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    collapsed: false
    number_sections: true
    toc_depth: 3
    #code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```

**IMPORTANT:** You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.

This is an `.Rmd` file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a \# in front of your text, it will create a top level-header.

# My first post

Last compiled: `r Sys.Date()`

Notice that whatever you define as a top level header, automatically gets put into the table of contents bar on the left. 

## Second level header

You can add more headers by adding more hashtags. These won't be put into the table of contents

### third level header

Here's an even lower level header

# My second post (note the order)

Last compiled: `r Sys.Date()`

I'm writing this tutorial going from the top down. And, this is how it will be printed. So, notice the second post is second in the list. If you want your most recent post to be at the top, then make a new post starting at the top. If you want the oldest first, do, then keep adding to the bottom

# Adding R stuff

So far this is just a blog where you can write in plain text and serve your writing to a webpage. One of the main purposes of this lab journal is to record your progress learning R. The reason I am asking you to use this process is because you can both make a website, and a lab journal, and learn R all in R-studio. This makes everything really convenient and in the same place. 

So, let's say you are learning how to make a histogram in R. For example, maybe you want to sample 100 numbers from a normal distribution with mean = 0, and standard deviation = 1, and then you want to plot a histogram. You can do this right here by using an r code block, like this:

```{r}
samples <- rnorm(100, mean=0, sd=1)
hist(samples)
```

When you knit this R Markdown document, you will see that the histogram is printed to the page, along with the R code. This document can be set up to hide the R code in the webpage, just delete the comment (hashtag) from the cold folding option in the yaml header up top. For purposes of letting yourself see the code, and me see the code, best to keep it the way that it is. You'll learn that all of these things and more can be customized in each R code block.

<!--chapter:end:07_class_notes.Rmd-->

---
title: "Resources · TUHH Business Analytics R Programming Guide"
output: html_document
---

<style type="text/css">
.title {
  display: none;
}
</style>
<div class="row" style="padding-top: 30px;">

# Resources
### R and R-Studio

[R](http://www.r-project.org) is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is [R-studio](http://www.rstudio.com), that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual []().

### Additional R resources

1. Google is great, Google your problem
2. [Stackoverflow](https://stackoverflow.com) is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways.
3. This is the definitive guide for all things R Markdown (you will find this very useful as you get better at this skill): [https://bookdown.org/yihui/rmarkdown/](https://bookdown.org/yihui/rmarkdown/)

<!--chapter:end:08_links.Rmd-->

